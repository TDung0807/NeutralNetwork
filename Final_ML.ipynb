{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "5466fZIV6Hh6",
        "p2L_d0MLCIIt",
        "q3IZ8qss6OQV"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Requiment A:\n",
        "  Discuss different optimizers used in training Neural Networks. Describe the advantages and disadvantages of these methods. Compare these methods for the same problem (using the same dataset)."
      ],
      "metadata": {
        "id": "5466fZIV6Hh6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Thoery"
      ],
      "metadata": {
        "id": "AAWZfEQvvTA7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Sumary"
      ],
      "metadata": {
        "id": "iGvjkbPxvspM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### SGD(Stochastic Gradient Descent)"
      ],
      "metadata": {
        "id": "OKBcQpvJxCzD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Tổng quan:**\n",
        "\n",
        "Stochastic Gradient Descent (SGD) là một biến thể của Gradient Descent trong đó các tham số của mô hình được cập nhật sau khi tính toán gradient của hàm mất mát trên một mẫu đơn lẻ hoặc một tập con nhỏ (mini-batch) của dữ liệu huấn luyện thay vì toàn bộ tập dữ liệu. Điều này giúp SGD trở nên rất nhanh và hiệu quả trong việc xử lý các bộ dữ liệu lớn và không đồng nhất. SGD giúp cải thiện khả năng hội tụ bằng cách thêm độ nhiễu vào quá trình tối ưu hóa, cho phép nó thoát khỏi các cực tiểu cục bộ."
      ],
      "metadata": {
        "id": "gJNLHKd4yuwI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Trường hợp nên dùng:**\n",
        "Dữ liệu lớn và các tình huống học online.\n",
        "\n",
        "1.   Large Datasets:\n",
        "Khi làm việc với các bộ dữ liệu lớn, việc tính toán độ dốc của toàn bộ bộ dữ liệu trở nên tốn kém về mặt tính toán. Trong trường hợp này, SGD tính toán độ dốc bằng cách chỉ sử dụng một phần của bộ dữ liệu, làm cho việc tính toán trở nên khả thi về mặt tính toán.\n",
        "2.   Online learning:\n",
        "Trong các tình huống mà dữ liệu mới liên tục đến hoặc phân phối dữ liệu thay đổi liên tục, SGD được ưa chuộng. Nó cập nhật các tham số của mô hình sau khi xử lý mỗi điểm dữ liệu hoặc một lô nhỏ dữ liệu, cho phép mô hình thích ứng nhanh chóng với các thay đổi.\n",
        "\n"
      ],
      "metadata": {
        "id": "2vNoAfFkzqv_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Ưu điểm:**\n",
        "*   Hiệu quả về mặt tính toán và cập nhật nhanh.\n",
        "*   Có thể thoát khỏi các cực tiểu cục bộ do nhiễu trong các bản cập nhật.\n",
        "\n",
        "**Nhược điểm:**\n",
        "*   Độ biến thiên cao trong các bản cập nhật dẫn đến các gradient nhiễu.\n",
        "*   Có thể hội tụ chậm và cần nhiều vòng lặp.\n",
        "\n"
      ],
      "metadata": {
        "id": "Uy7v2IHrxWyM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Adam(Adaptive Moment Estimation)"
      ],
      "metadata": {
        "id": "SZAUx7itxFvY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Tổng quan:**\n",
        "\n",
        "Adam (Adaptive Moment Estimation) là một thuật toán tối ưu hóa thích ứng kết hợp các khía cạnh tốt nhất của hai bộ tối ưu hóa phổ biến là Adagrad và RMSProp. Adam sử dụng cả moment bậc nhất (trung bình động của các gradient) và moment bậc hai (trung bình động của các gradient bình phương) để điều chỉnh tốc độ học. Điều này cho phép Adam tự động điều chỉnh tốc độ học cho mỗi tham số, giúp nó hoạt động hiệu quả trong nhiều tình huống khác nhau, bao gồm các vấn đề phi tuyến tính và có độ nhiễu cao. Adam thường được sử dụng với các thiết lập mặc định (như learning rate = 0.001, beta1 = 0.9, beta2 = 0.999) và vẫn mang lại hiệu suất tốt.\n"
      ],
      "metadata": {
        "id": "rgB7kgzty5R_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Trường hợp nên dùng:**\n",
        "\n",
        "1.   **Large dataset:**\n",
        "Tương tự như SGD, Adam hiệu quả cho các bộ dữ liệu lớn. Cơ chế tỷ lệ học thích ứng của nó cho phép nó xử lý các bộ dữ liệu quy mô lớn một cách hiệu quả bằng cách điều chỉnh tỷ lệ học cho mỗi tham số riêng biệt dựa trên độ lớn của độ dốc của chúng.\n",
        "\n",
        "2.   **Sparse Gradients:**\n",
        "Trong các tình huống mà độ dốc rời rạc, như các nhiệm vụ xử lý ngôn ngữ tự nhiên hoặc cơ chế chú ý trong mạng neural, Adam có thể điều chỉnh tỷ lệ học một cách thích ứng cho mỗi tham số, ngăn chặn sự hội tụ chậm chạp\n",
        "3. **Non-Convex Optimization:**\n",
        "Adam thích hợp cho các vấn đề tối ưu hóa không lồi thường gặp khi huấn luyện mạng neural. Tính chất thích ứng của nó cho phép nó điều hướng qua các định cư mất mát phức tạp và thoát khỏi các điểm cực tiểu địa phương một cách hiệu quả.\n",
        "\n"
      ],
      "metadata": {
        "id": "n4YiAUFHzyEK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Ưu điểm:**\n",
        "*   Hiệu quả và yêu cầu ít bộ nhớ.\n",
        "*   Thường hoạt động tốt với việc tinh chỉnh tham số tối thiểu.\n",
        "\n",
        "**Nhược điểm:**\n",
        "\n",
        "*   Có thể ít dễ hiểu hơn do độ phức tạp của nó.\n",
        "*   Yêu cầu tinh chỉnh nhiều tham số (tốc độ học, beta1, beta2)."
      ],
      "metadata": {
        "id": "aCc3AUZI0K9C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####  RMSProp(Root Mean Square Propagation)"
      ],
      "metadata": {
        "id": "RAZjXymyxJkp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Tổng quan:**\n",
        "\n",
        "RMSProp là một thuật toán tối ưu hóa thích ứng được thiết kế để khắc phục nhược điểm của Adagrad, cụ thể là tốc độ học giảm quá nhanh. RMSProp sử dụng một trung bình động của các gradient bình phương để chuẩn hóa gradient, từ đó điều chỉnh tốc độ học cho mỗi tham số theo cách không làm cho tốc độ học trở nên quá nhỏ. RMSProp thường được sử dụng trong các mạng nơ-ron sâu và các vấn đề có độ biến thiên cao, và thường sử dụng giá trị mặc định của hệ số suy giảm (decay rate) khoảng 0.9."
      ],
      "metadata": {
        "id": "s8fAcsiwzB5J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Trường hợp nên dùng:**\n",
        "\n",
        "\n",
        "1.    **Recurrent Neural Networks(RNNs):**\n",
        "RMSProp thường được sử dụng với RNNs, đặc biệt là trong các tình huống có sự phụ thuộc dài hạn. Tốc độ học thích ứng giúp giải quyết vấn đề gradient biến mất/phình to.\n",
        "\n",
        "2.   **Natural Language Processing (NLP):**\n",
        "Trong các nhiệm vụ như mô hình ngôn ngữ, phân tích cảm xúc hoặc dịch máy nơi dữ liệu có thể thưa hoặc có tần suất khác nhau của từ, RMSProp có thể hữu ích do cơ chế tốc độ học thích ứng của nó.\n",
        "\n",
        "3. **Sparse Data:**\n",
        "Khi xử lý các tập dữ liệu mà chỉ một phần nhỏ các đặc trưng có giá trị khác không, như trong các hệ thống gợi ý hoặc một số loại dữ liệu hình ảnh, RMSProp có thể giúp cập nhật hiệu quả các tham số mô hình mà không quá thích ứng với các đặc trưng nhiễu hoặc không liên quan.\n"
      ],
      "metadata": {
        "id": "IRpet3rp1OhQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ưu điểm:\n",
        "\n",
        "*   Ngăn tốc độ học không trở nên quá nhỏ.\n",
        "*   Phù hợp với các vấn đề không ổn định.\n",
        "\n",
        "Nhược điểm:\n",
        "\n",
        "*   Yêu cầu tinh chỉnh tham số suy giảm.\n",
        "*   Phức tạp hơn để thực hiện so với Adagrad."
      ],
      "metadata": {
        "id": "lZmbMe9i1Q4j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### AdaGrad(Adaptive Gradient Algorithm)"
      ],
      "metadata": {
        "id": "xIgctKH5xORS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Tổng quan:**\n",
        "\n",
        "Adagrad là một thuật toán tối ưu hóa điều chỉnh tốc độ học cho mỗi tham số riêng lẻ dựa trên tổng các gradient bình phương của tham số đó từ đầu quá trình huấn luyện. Điều này giúp các tham số hiếm gặp (thường có gradient lớn hơn) có tốc độ học nhanh hơn, trong khi các tham số thường gặp có tốc độ học chậm hơn. Adagrad đặc biệt hữu ích trong các vấn đề với dữ liệu thưa (sparse data) và các tình huống mà tầm quan trọng của các đặc trưng thay đổi đáng kể trong suốt quá trình huấn luyện. Tuy nhiên, một nhược điểm lớn của Adagrad là tốc độ học của nó có thể trở nên quá nhỏ sau một thời gian, dẫn đến việc mô hình ngừng học sớm."
      ],
      "metadata": {
        "id": "MO6vxvH1zGW-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Trường hợp nên dùng:**\n",
        "\n",
        "\n",
        "1.   **Sparse Data:**\n",
        "AdaGrad hoạt động tốt trong các tình huống có dữ liệu thưa, nơi một số đặc trưng có thể có độ dốc lớn hơn nhiều so với các đặc trưng khác. Nó tự động điều chỉnh tốc độ học cho mỗi tham số dựa trên tần suất của các cập nhật của nó, điều này có thể có lợi khi xử lý các đặc trưng thưa.\n",
        "2.   **Natural Language Processing(NLP):**\n",
        " Trong các nhiệm vụ NLP như mô hình ngôn ngữ, phân loại văn bản hoặc phân tích cảm xúc, AdaGrad có thể hiệu quả vì khả năng xử lý tần suất biến thiên của các từ và tính thưa của dữ liệu ngôn ngữ.\n",
        "3. **Feature Engineering:**\n",
        "Khi xử lý các không gian đặc trưng có chiều cao và tương tác đặc trưng phức tạp, AdaGrad có thể giúp tự động điều chỉnh tốc độ học cho các đặc trưng khác nhau dựa trên sự quan trọng của chúng, có thể dẫn đến sự hội tụ nhanh hơn và tổng quát hóa tốt hơn.\n"
      ],
      "metadata": {
        "id": "UBM4zuDz0pRT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Ưu điểm:**\n",
        "\n",
        "*   Tự động điều chỉnh tốc độ học.\n",
        "*   Tốt cho dữ liệu thưa.\n",
        "\n",
        "**Nhược điểm:**\n",
        "\n",
        "*   Tốc độ học có thể trở nên quá nhỏ.\n",
        "*   Có thể dừng học sớm."
      ],
      "metadata": {
        "id": "QXo_LCDp0tZa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Coding"
      ],
      "metadata": {
        "id": "p2L_d0MLCIIt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Library"
      ],
      "metadata": {
        "id": "d7Fnczpn6Kwv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras"
      ],
      "metadata": {
        "id": "JS5jLxdy40gu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data generating"
      ],
      "metadata": {
        "id": "idn5SLnJ6bEU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()"
      ],
      "metadata": {
        "id": "j9wyeu8f6df1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train = x_train / 255.0\n",
        "x_test = x_test / 255.0"
      ],
      "metadata": {
        "id": "Pr_ZcsIV6uSX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Neutral Network"
      ],
      "metadata": {
        "id": "q3IZ8qss6OQV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Implement Neutral Network"
      ],
      "metadata": {
        "id": "lJxUNugOKXe4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_model():\n",
        "    model = keras.Sequential([\n",
        "        # Input layer: flatten 28x28 images to 1D\n",
        "        keras.layers.Flatten(input_shape=(28, 28)),\n",
        "        # Hidden layer: 128 neurons, ReLU activation\n",
        "        keras.layers.Dense(128, activation='relu'),\n",
        "        # Output layer: 10 neurons for 10 classes, softmax activation\n",
        "        keras.layers.Dense(10, activation='softmax')\n",
        "    ])\n",
        "    return model"
      ],
      "metadata": {
        "id": "W6Icco_94w_I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### SGD Optimizer"
      ],
      "metadata": {
        "id": "1M9NXEhzKbHg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_sgd = create_model()\n",
        "model_sgd.compile(optimizer='sgd',\n",
        "                  loss='sparse_categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "# Train the model with SGD optimizer\n",
        "model_sgd.fit(x_train, y_train, epochs=10, batch_size=32)\n",
        "\n",
        "# Evaluate the model with SGD optimizer\n",
        "sgd_eval = model_sgd.evaluate(x_test, y_test)\n",
        "print(f'SGD - Test loss: {sgd_eval[0]}, Test accuracy: {sgd_eval[1]}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "okkw9Up6KggR",
        "outputId": "8b5b5254-2bbd-4215-f4ea-1f0e0249b386"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "1875/1875 [==============================] - 5s 2ms/step - loss: 0.6503 - accuracy: 0.8392\n",
            "Epoch 2/10\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.3412 - accuracy: 0.9043\n",
            "Epoch 3/10\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.2926 - accuracy: 0.9176\n",
            "Epoch 4/10\n",
            "1875/1875 [==============================] - 5s 2ms/step - loss: 0.2622 - accuracy: 0.9260\n",
            "Epoch 5/10\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.2390 - accuracy: 0.9330\n",
            "Epoch 6/10\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.2200 - accuracy: 0.9377\n",
            "Epoch 7/10\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.2039 - accuracy: 0.9433\n",
            "Epoch 8/10\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.1902 - accuracy: 0.9470\n",
            "Epoch 9/10\n",
            "1875/1875 [==============================] - 5s 2ms/step - loss: 0.1782 - accuracy: 0.9497\n",
            "Epoch 10/10\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.1676 - accuracy: 0.9527\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.1641 - accuracy: 0.9530\n",
            "SGD - Test loss: 0.16409817337989807, Test accuracy: 0.953000009059906\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Adam Optimizer"
      ],
      "metadata": {
        "id": "eTmgB6TSZk5n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create and compile the model with Adam optimizer\n",
        "model_adam = create_model()\n",
        "model_adam.compile(optimizer='adam',\n",
        "                   loss='sparse_categorical_crossentropy',\n",
        "                   metrics=['accuracy'])\n",
        "\n",
        "# Train the model with Adam optimizer\n",
        "model_adam.fit(x_train, y_train, epochs=10, batch_size=32)\n",
        "\n",
        "# Evaluate the model with Adam optimizer\n",
        "adam_eval = model_adam.evaluate(x_test, y_test)\n",
        "print(f'Adam - Test loss: {adam_eval[0]}, Test accuracy: {adam_eval[1]}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "NxrVxnkYZmxf",
        "outputId": "0c517abe-07dc-4e93-cfba-2555c4d86c55"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.2580 - accuracy: 0.9265\n",
            "Epoch 2/10\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.1147 - accuracy: 0.9661\n",
            "Epoch 3/10\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.0796 - accuracy: 0.9763\n",
            "Epoch 4/10\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.0603 - accuracy: 0.9817\n",
            "Epoch 5/10\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.0471 - accuracy: 0.9852\n",
            "Epoch 6/10\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.0374 - accuracy: 0.9883\n",
            "Epoch 7/10\n",
            "1875/1875 [==============================] - 7s 3ms/step - loss: 0.0298 - accuracy: 0.9910\n",
            "Epoch 8/10\n",
            "1875/1875 [==============================] - 9s 5ms/step - loss: 0.0244 - accuracy: 0.9925\n",
            "Epoch 9/10\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.0212 - accuracy: 0.9934\n",
            "Epoch 10/10\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.0178 - accuracy: 0.9944\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.0752 - accuracy: 0.9802\n",
            "Adam - Test loss: 0.07516637444496155, Test accuracy: 0.9801999926567078\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### RMSProp Optimizer"
      ],
      "metadata": {
        "id": "rt90dfPJZ5L2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create and compile the model with RMSProp optimizer\n",
        "model_rmsprop = create_model()\n",
        "model_rmsprop.compile(optimizer='rmsprop',\n",
        "                      loss='sparse_categorical_crossentropy',\n",
        "                      metrics=['accuracy'])\n",
        "\n",
        "# Train the model with RMSProp optimizer\n",
        "model_rmsprop.fit(x_train, y_train, epochs=10, batch_size=32)\n",
        "\n",
        "# Evaluate the model with RMSProp optimizer\n",
        "rmsprop_eval = model_rmsprop.evaluate(x_test, y_test)\n",
        "print(f'RMSProp - Test loss: {rmsprop_eval[0]}, Test accuracy: {rmsprop_eval[1]}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "1eNhxfa6Z77X",
        "outputId": "4a99a978-d0f8-46c4-aa22-78c3e26897ab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.2519 - accuracy: 0.9282\n",
            "Epoch 2/10\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.1141 - accuracy: 0.9663\n",
            "Epoch 3/10\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.0823 - accuracy: 0.9756\n",
            "Epoch 4/10\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.0650 - accuracy: 0.9813\n",
            "Epoch 5/10\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.0543 - accuracy: 0.9838\n",
            "Epoch 6/10\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.0456 - accuracy: 0.9867\n",
            "Epoch 7/10\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.0397 - accuracy: 0.9886\n",
            "Epoch 8/10\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.0338 - accuracy: 0.9898\n",
            "Epoch 9/10\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.0293 - accuracy: 0.9915\n",
            "Epoch 10/10\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.0253 - accuracy: 0.9928\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.0799 - accuracy: 0.9809\n",
            "RMSProp - Test loss: 0.07985502481460571, Test accuracy: 0.98089998960495\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### AdaGrad Optimizer"
      ],
      "metadata": {
        "id": "Qx_YxQAeux3j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create and compile the model with RMSProp optimizer\n",
        "model_adagrad = create_model()\n",
        "model_adagrad.compile(optimizer='adagrad',\n",
        "                      loss='sparse_categorical_crossentropy',\n",
        "                      metrics=['accuracy'])\n",
        "\n",
        "# Train the model with RMSProp optimizer\n",
        "model_adagrad.fit(x_train, y_train, epochs=10, batch_size=32)\n",
        "\n",
        "# Evaluate the model with RMSProp optimizer\n",
        "adagrad_eval = model_adagrad.evaluate(x_test, y_test)\n",
        "print(f'AdaGrad - Test loss: {adagrad_eval[0]}, Test accuracy: {adagrad_eval[1]}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DcdhhCBnuzzV",
        "outputId": "4d02f39e-718f-41b5-c674-a7c7f84ec937",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 1.2456 - accuracy: 0.7030\n",
            "Epoch 2/10\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.6348 - accuracy: 0.8508\n",
            "Epoch 3/10\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.5115 - accuracy: 0.8714\n",
            "Epoch 4/10\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.4547 - accuracy: 0.8819\n",
            "Epoch 5/10\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.4205 - accuracy: 0.8888\n",
            "Epoch 6/10\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.3967 - accuracy: 0.8935\n",
            "Epoch 7/10\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.3788 - accuracy: 0.8978\n",
            "Epoch 8/10\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.3647 - accuracy: 0.9008\n",
            "Epoch 9/10\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.3531 - accuracy: 0.9033\n",
            "Epoch 10/10\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.3432 - accuracy: 0.9058\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.3235 - accuracy: 0.9138\n",
            "AdaGrad - Test loss: 0.32346445322036743, Test accuracy: 0.9138000011444092\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Result Each Optimizer"
      ],
      "metadata": {
        "id": "7pKQEUSsaUuu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'SGD - Test loss: {sgd_eval[0]}, Test accuracy: {sgd_eval[1]}')\n",
        "print(f'Adam - Test loss: {adam_eval[0]}, Test accuracy: {adam_eval[1]}')\n",
        "print(f'RMSProp - Test loss: {rmsprop_eval[0]}, Test accuracy: {rmsprop_eval[1]}')\n",
        "print(f'AdaGrad - Test loss: {adagrad_eval[0]}, Test accuracy: {adagrad_eval[1]}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oUBnARSoac6M",
        "outputId": "838f3fc8-35ee-497d-bf57-a67854fd91e8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SGD - Test loss: 0.16409817337989807, Test accuracy: 0.953000009059906\n",
            "Adam - Test loss: 0.07516637444496155, Test accuracy: 0.9801999926567078\n",
            "RMSProp - Test loss: 0.07985502481460571, Test accuracy: 0.98089998960495\n",
            "AdaGrad - Test loss: 0.32346445322036743, Test accuracy: 0.9138000011444092\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Requiment B:\n"
      ],
      "metadata": {
        "id": "KH9qaXH2T531"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Theory"
      ],
      "metadata": {
        "id": "-zp4ftUqNPiB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Backpropagation"
      ],
      "metadata": {
        "id": "UE4P39dxO6CJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Thuật toán Backpropagation là một phương pháp tính đạo hàm ngược (reverse-mode differentiation) được áp dụng trong việc huấn luyện mô hình mạng nơ-ron (neural network). Là một ứng dụng của tối ưu hóa gradient descent và áp dụng quy tắc chuỗi (chain rule) để tính toán gradient của hàm mất mát (loss function) đối với từng trọng số (weight)\n",
        "\n",
        " Đây là thuật toán cốt lõi giúp mô hình học sâu thực thi hiệu quả, làm cho quá trình tối ưu nhanh hơn hàng triệu lần so với phương pháp truyền thống."
      ],
      "metadata": {
        "id": "_C9S8JfzNX4s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Reverse-mode difference"
      ],
      "metadata": {
        "id": "-zibPGmfOp5q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Phương pháp này tính gradient của một hàm có giá trị vô hướng đối với các đầu vào của nó. Điều này đặc biệt hữu ích khi hàm có nhiều đầu vào và một đầu ra duy nhất, thường gặp trong các mô hình học máy, nơi cần tính gradient của hàm mất mát đối với các tham số của mô hình."
      ],
      "metadata": {
        "id": "xQeHnm69OwtG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step"
      ],
      "metadata": {
        "id": "VXv5oIA6PYZx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Initial"
      ],
      "metadata": {
        "id": "YPaemOPMYqnw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "$ x $ is input vector\n",
        "\n",
        "$ y $ is output vector\n",
        "\n",
        "$ W^l $ be the weight for layer $l$\n",
        "\n",
        "$ b^l $ be the weight for layer $l$\n",
        "\n",
        "$ a^l $ be the weight for layer $l$"
      ],
      "metadata": {
        "id": "H_H8yKAAYvBD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Forward"
      ],
      "metadata": {
        "id": "cPB13de-PcGu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The activation of layer $ l $ is given by:\n",
        "\n",
        "$a^l = f(z^l)$\n",
        "\n",
        "Where $z^l$:\n",
        "\n",
        "$z^l = W^la^{l-1} + b^l$\n",
        "\n",
        "$f$ is the activation function(relu, sigmoid,...)\n"
      ],
      "metadata": {
        "id": "IQu86m_hZjyO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Loss function"
      ],
      "metadata": {
        "id": "1OTF4NcGXi_y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "$$ L = \\sum_{i=0}^{\\infty}\\frac{1}{2}(y - |y|)^2 $$\n",
        "\n",
        "$y$ is target value\n",
        "\n",
        "$|y|$ is predict value"
      ],
      "metadata": {
        "id": "Erb_QxCZXm0Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Backward"
      ],
      "metadata": {
        "id": "aWhOQhZePems"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To minimize the loss, we need to compute the gradients of the loss function with respect to the weights $W^l$ and biases $b^l$\n",
        "\n",
        "Using the chain rule, the gradient of the loss with respect to the weights in the last layer $L$ is:\n",
        "\n",
        "$$\\frac{\\delta L}{\\delta{W^l}} = \\frac{\\delta L}{\\delta{a^L}}.\\frac{\\delta(a^L)}{\\delta{z^L}}.\\frac{\\delta(z^L)}{\\delta{W^l}}$$"
      ],
      "metadata": {
        "id": "US4qMjeXYVfw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Update Weight($n$ is learning rate):\n",
        "$$W^l \\leftarrow W^l - n\\frac{\\delta L}{\\delta{W^l}}$$\n",
        "\n",
        "$$b^l \\leftarrow b^l - n\\frac{\\delta L}{\\delta{b^l}}$$"
      ],
      "metadata": {
        "id": "BpQaCbO-YZqp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Coding"
      ],
      "metadata": {
        "id": "FOOuXV8WNORW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Library"
      ],
      "metadata": {
        "id": "pblzGYH7I5Lw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np"
      ],
      "metadata": {
        "id": "duPgLjXYI1tA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Support function"
      ],
      "metadata": {
        "id": "eLJaUd7EMNIK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Sigmoid function: applied to the input to produce an output that lies between 0 and 1\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "# compute gradient of loss function, optimized weight\n",
        "def sigmoid_derivative(x):\n",
        "    return x * (1 - x)\n",
        "\n",
        "# Loss function (Mean Squared Error)\n",
        "def mse_loss(y_true, y_pred):\n",
        "    return np.mean((y_true - y_pred) ** 2)"
      ],
      "metadata": {
        "id": "NkjhsfPtIxkl"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Class"
      ],
      "metadata": {
        "id": "6W95KbcgMUxB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class NeuralNetwork:\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        # Initialize weights and biases\n",
        "        self.w1 = np.random.randn(input_size, hidden_size)\n",
        "        self.b1 = np.zeros((1, hidden_size))\n",
        "        self.w2 = np.random.randn(hidden_size, output_size)\n",
        "        self.b2 = np.zeros((1, output_size))\n",
        "\n",
        "    # Computes the output of the neural network\n",
        "    def forward(self, X):\n",
        "        self.z1 = np.dot(X, self.w1) + self.b1\n",
        "        self.a1 = sigmoid(self.z1)\n",
        "        self.z2 = np.dot(self.a1, self.w2) + self.b2\n",
        "        self.a2 = sigmoid(self.z2)\n",
        "        return self.a2\n",
        "    # Updates the weights and biases based on the error between the predicted output\n",
        "    def backward(self, X, y, output, learning_rate):\n",
        "\n",
        "        # Compute the loss derivative with respect to output\n",
        "        loss_grad = (output - y) / y.size\n",
        "        # start Backpropagation\n",
        "        d_z2 = loss_grad * sigmoid_derivative(output)\n",
        "        d_w2 = np.dot(self.a1.T, d_z2)\n",
        "        d_b2 = np.sum(d_z2, axis=0, keepdims=True)\n",
        "\n",
        "        d_a1 = np.dot(d_z2, self.w2.T)\n",
        "        d_z1 = d_a1 * sigmoid_derivative(self.a1)\n",
        "        d_w1 = np.dot(X.T, d_z1)\n",
        "        d_b1 = np.sum(d_z1, axis=0, keepdims=True)\n",
        "        # end backpropagation\n",
        "\n",
        "\n",
        "        # Update weights and biases\n",
        "        self.w1 -= learning_rate * d_w1\n",
        "        self.b1 -= learning_rate * d_b1\n",
        "        self.w2 -= learning_rate * d_w2\n",
        "        self.b2 -= learning_rate * d_b2\n",
        "\n",
        "    def train(self, X, y, epochs, learning_rate):\n",
        "        for epoch in range(epochs):\n",
        "            output = self.forward(X)\n",
        "            self.backward(X, y, output, learning_rate)\n",
        "            if (epoch+1) % 100 == 0:\n",
        "                loss = mse_loss(y, output)\n",
        "                # print(f'Epoch {epoch+1}, Loss: {loss}')"
      ],
      "metadata": {
        "id": "zoF-rUyxwEAa"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training data (XOR problem)\n",
        "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
        "y = np.array([[0], [1], [1], [0]])\n",
        "\n",
        "# Create the neural network\n",
        "nn = NeuralNetwork(input_size=2, hidden_size=2, output_size=1)\n",
        "\n",
        "# Train the neural network\n",
        "nn.train(X, y, epochs=100000, learning_rate=0.1)\n",
        "\n",
        "# Test the neural network\n",
        "result = nn.forward(X)\n",
        "print(\"Result:\")\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TFwVhAibMZXN",
        "outputId": "4faa7c48-4d81-474b-f524-27e60734a48b"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result:\n",
            "[[0.02327843]\n",
            " [0.97508931]\n",
            " [0.97515851]\n",
            " [0.03092309]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Reference"
      ],
      "metadata": {
        "id": "JmDrTycBd3s1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tiếng Việt"
      ],
      "metadata": {
        "id": "Nf2l0kygd6yB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Wikipedia: [link](https://vi.wikipedia.org/wiki/Truy%E1%BB%81n_ng%C6%B0%E1%BB%A3c)\n",
        "\n",
        "2. [NN] Về lan truyền ngược - Backpropagation: [link](https://dominhhai.github.io/vi/2018/04/nn-bp/)\n",
        "\n",
        "3. Giải thích chi tiết thuật toán BackPropagation: [link](https://nguyentruonglong.net/giai-thich-chi-tiet-thuat-toan-backpropagation.html)"
      ],
      "metadata": {
        "id": "dDGPApO6eCRd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tiếng Anh"
      ],
      "metadata": {
        "id": "9NFxon_4d-aR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Understanding Optimizers for training Deep Learning Models [link](https://medium.com/game-of-bits/understanding-optimizers-for-training-deep-learning-models-694c071b5b70#:~:text=SGD%20optimizer%20works%20with%20a,modification%20of%20the%20SGD%20algorithm.)\n",
        "2. Wikipedia: [link](https://en.wikipedia.org/wiki/Backpropagation)\n",
        "3. Calculus on Computational Graphs: Backprop: [link](https://colah.github.io/posts/2015-08-Backprop/)\n",
        "4. Backpropagation from scratch with Python: [link](https://pyimagesearch.com/2021/05/06/backpropagation-from-scratch-with-python/)"
      ],
      "metadata": {
        "id": "imVaSsOCeCtR"
      }
    }
  ]
}